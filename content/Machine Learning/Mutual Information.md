# What is it?

*Mutual Information* between two variables describes the **extent to which knowledge of one quantity reduces uncertainty about the other**. In this case, *uncertainty* refers to the [[Entropy]] of the variables. In other words, if you **know the value of a feature**, how confident would you be about **predicting a target**?

It may sound very similar to [[Correlation]], but *Mutual Information* is not restricted only to linear relationships. It **captures any kind of relationship between variables**, being **linear or non-linear**, and they can be both **continuous and discrete** variables.

It's given by the formula:

$$
I(X;Y) = \sum
$$
___
# Mutual Information and Entropy


![[mutual information.png]]